{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](../data/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Machine Learning Accelerator - Natural Language Processing - Lecture 1</a>\n",
    "\n",
    "## Text Processing\n",
    "\n",
    "In this notebook, we go over some simple techniques to clean and prepare text data for modeling with machine learning.\n",
    "\n",
    "1. <a href=\"#1\">Simple text cleaning processes</a>\n",
    "2. <a href=\"#2\">Lexicon-based text processing</a>\n",
    "    * Stop words removal \n",
    "    * Stemming   \n",
    "    * Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in c:\\users\\ed\\.conda\\envs\\aws-nlp\\lib\\site-packages (3.25.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel in c:\\users\\ed\\.conda\\envs\\aws-nlp\\lib\\site-packages (0.37.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [116 lines of output]\n",
      "  C:\\Users\\Ed\\.conda\\envs\\aws-nlp\\lib\\site-packages\\setuptools\\installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\n",
      "    warnings.warn(\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\n",
      "  copying src\\gluonnlp\\base.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\n",
      "  copying src\\gluonnlp\\_constants.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\n",
      "  copying src\\gluonnlp\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\calibration\n",
      "  copying src\\gluonnlp\\calibration\\collector.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\calibration\n",
      "  copying src\\gluonnlp\\calibration\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\calibration\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\candidate_sampler.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\conll.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\dataloader.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\dataset.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\datasetloader.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\glue.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\intent_slot.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\question_answering.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\registry.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\sampler.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\sentiment.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\stream.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\super_glue.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\transforms.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\translation.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\utils.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\word_embedding_evaluation.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  copying src\\gluonnlp\\data\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\embedding\n",
      "  copying src\\gluonnlp\\embedding\\evaluation.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\embedding\n",
      "  copying src\\gluonnlp\\embedding\\token_embedding.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\embedding\n",
      "  copying src\\gluonnlp\\embedding\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\embedding\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\initializer\n",
      "  copying src\\gluonnlp\\initializer\\initializer.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\initializer\n",
      "  copying src\\gluonnlp\\initializer\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\initializer\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\loss\n",
      "  copying src\\gluonnlp\\loss\\activation_regularizer.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\loss\n",
      "  copying src\\gluonnlp\\loss\\label_smoothing.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\loss\n",
      "  copying src\\gluonnlp\\loss\\loss.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\loss\n",
      "  copying src\\gluonnlp\\loss\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\loss\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\metric\n",
      "  copying src\\gluonnlp\\metric\\length_normalized_loss.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\metric\n",
      "  copying src\\gluonnlp\\metric\\masked_accuracy.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\metric\n",
      "  copying src\\gluonnlp\\metric\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\metric\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\attention_cell.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\bert.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\bilm_encoder.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\block.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\convolutional_encoder.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\elmo.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\highway.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\language_model.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\lstmpcellwithclip.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\parameter.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\sampled_block.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\seq2seq_encoder_decoder.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\sequence_sampler.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\transformer.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\translation.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\utils.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  copying src\\gluonnlp\\model\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\optimizer\n",
      "  copying src\\gluonnlp\\optimizer\\bert_adam.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\optimizer\n",
      "  copying src\\gluonnlp\\optimizer\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\optimizer\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\utils\n",
      "  copying src\\gluonnlp\\utils\\files.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\utils\n",
      "  copying src\\gluonnlp\\utils\\parallel.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\utils\n",
      "  copying src\\gluonnlp\\utils\\parameter.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\utils\n",
      "  copying src\\gluonnlp\\utils\\version.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\utils\n",
      "  copying src\\gluonnlp\\utils\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\utils\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\vocab\n",
      "  copying src\\gluonnlp\\vocab\\bert.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\vocab\n",
      "  copying src\\gluonnlp\\vocab\\elmo.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\vocab\n",
      "  copying src\\gluonnlp\\vocab\\subwords.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\vocab\n",
      "  copying src\\gluonnlp\\vocab\\vocab.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\vocab\n",
      "  copying src\\gluonnlp\\vocab\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\vocab\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\data\\batchify\n",
      "  copying src\\gluonnlp\\data\\batchify\\batchify.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\\batchify\n",
      "  copying src\\gluonnlp\\data\\batchify\\embedding.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\\batchify\n",
      "  copying src\\gluonnlp\\data\\batchify\\language_model.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\\batchify\n",
      "  copying src\\gluonnlp\\data\\batchify\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\\batchify\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\data\\corpora\n",
      "  copying src\\gluonnlp\\data\\corpora\\google_billion_word.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\\corpora\n",
      "  copying src\\gluonnlp\\data\\corpora\\large_text_compression_benchmark.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\\corpora\n",
      "  copying src\\gluonnlp\\data\\corpora\\wikitext.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\\corpora\n",
      "  copying src\\gluonnlp\\data\\corpora\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\\corpora\n",
      "  creating build\\lib.win-amd64-cpython-39\\gluonnlp\\model\\train\n",
      "  copying src\\gluonnlp\\model\\train\\cache.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\\train\n",
      "  copying src\\gluonnlp\\model\\train\\embedding.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\\train\n",
      "  copying src\\gluonnlp\\model\\train\\language_model.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\\train\n",
      "  copying src\\gluonnlp\\model\\train\\__init__.py -> build\\lib.win-amd64-cpython-39\\gluonnlp\\model\\train\n",
      "  running egg_info\n",
      "  writing src\\gluonnlp.egg-info\\PKG-INFO\n",
      "  writing dependency_links to src\\gluonnlp.egg-info\\dependency_links.txt\n",
      "  writing requirements to src\\gluonnlp.egg-info\\requires.txt\n",
      "  writing top-level names to src\\gluonnlp.egg-info\\top_level.txt\n",
      "  reading manifest file 'src\\gluonnlp.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no files found matching '*.py' under directory 'gluonnlp'\n",
      "  warning: no previously-included files matching '*' found under directory 'tests'\n",
      "  warning: no previously-included files matching '*' found under directory 'scripts'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'src\\gluonnlp.egg-info\\SOURCES.txt'\n",
      "  copying src\\gluonnlp\\data\\fast_bert_tokenizer.pyx -> build\\lib.win-amd64-cpython-39\\gluonnlp\\data\n",
      "  running build_ext\n",
      "  cythoning src/gluonnlp/data/fast_bert_tokenizer.pyx to src/gluonnlp/data\\fast_bert_tokenizer.c\n",
      "  c:\\users\\ed\\appdata\\local\\temp\\pip-install-bfvu73q0\\gluonnlp_62ef9a58613f40d6926420c97bd7b8df\\.eggs\\cython-0.29.33-py3.9.egg\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\Ed\\AppData\\Local\\Temp\\pip-install-bfvu73q0\\gluonnlp_62ef9a58613f40d6926420c97bd7b8df\\src\\gluonnlp\\data\\fast_bert_tokenizer.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  building 'gluonnlp.data.fast_bert_tokenizer' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for gluonnlp\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 160, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 247, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_internal\\commands\\install.py\", line 494, in run\n",
      "    installed = install_given_reqs(\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_internal\\req\\__init__.py\", line 73, in install_given_reqs\n",
      "    requirement.install(\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_internal\\req\\req_install.py\", line 792, in install\n",
      "    install_wheel(\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_internal\\operations\\install\\wheel.py\", line 729, in install_wheel\n",
      "    _install_wheel(\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_internal\\operations\\install\\wheel.py\", line 646, in _install_wheel\n",
      "    generated_console_scripts = maker.make_multiple(scripts_to_generate)\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_vendor\\distlib\\scripts.py\", line 436, in make_multiple\n",
      "    filenames.extend(self.make(specification, options))\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_internal\\operations\\install\\wheel.py\", line 427, in make\n",
      "    return super().make(specification, options)\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_vendor\\distlib\\scripts.py\", line 425, in make\n",
      "    self._make_script(entry, filenames, options=options)\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_vendor\\distlib\\scripts.py\", line 325, in _make_script\n",
      "    self._write_script(scriptnames, shebang, script, filenames, ext)\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_vendor\\distlib\\scripts.py\", line 249, in _write_script\n",
      "    launcher = self._get_launcher('t')\n",
      "  File \"C:\\Users\\Ed\\AppData\\Roaming\\Python\\Python39\\site-packages\\pip\\_vendor\\distlib\\scripts.py\", line 404, in _get_launcher\n",
      "    raise ValueError(msg)\n",
      "ValueError: Unable to find resource t64.exe in package pip._vendor.distlib\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Simple text cleaning processes</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this section, we will do some general purpose text cleaning. The following methods for cleaning can be extended depending on the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first lowercase our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get rid of leading/trailing whitespace with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove HTML tags/markups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = re.compile('<.*?>').sub('', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace punctuation with space. Be careful with this one, depending on the application, punctuations can actually be useful. For example positive vs negative meanining of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove extra space and tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = re.sub('\\s+', ' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Lexicon-based text processing</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We saw some general purpose text pre-processing methods in the previous section. Lexicon based methods are usually applied after the common text processing methods. They are used to normalize sentences in our dataset. By normalization, here, we mean putting words into a similar format that will also enhace similarities (if any) between sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install some packages for this example. Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop word removal\n",
    "There can be some words in our sentences that occur very frequently and don't contribute too much to the overall meaning of the sentences. We usually have a list of these words and remove them from each our sentences. For example: \"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\" in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a tokenizer from the NLTK library\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "# Stop word lists can be adjusted for your problem\n",
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(text)\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "text = \" \".join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Stemming is a rule-based system to convert words into their root form. It removes suffixes from words. This helps us enhace similarities (if any) between sentences. \n",
    "\n",
    "Example:\n",
    "\n",
    "\"jumping\", \"jumped\" -> \"jump\"\n",
    "\n",
    "\"cars\" -> \"car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a tokenizer and stemmer from the NLTK library\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "stemmed_sentence = []\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(text)\n",
    "for w in words:\n",
    "    # Stem the word/token\n",
    "    stemmed_sentence.append(snow.stem(w))\n",
    "stemmed_text = \" \".join(stemmed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above that stemming operation is NOT perfect. We have mistakes such as \"messag\", \"involv\", \"adjac\". It is a rule based method that sometimes mistakely remove suffixes from words. Nevertheless, it runs fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "If we are not satisfied with the result of stemming, we can use the Lemmatization instead. It usually requires more work, but gives better results. As mentioned in the class, lemmatization needs to know the correct word position tags such as \"noun\", \"verb\", \"adjective\", etc. and we will use another NLTK function to feed this information to the lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary functions\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "lemmatized_sentence = []\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(text)\n",
    "# Get position tags\n",
    "word_pos_tags = nltk.pos_tag(words)\n",
    "# Map the position tag and lemmatize the word/token\n",
    "for idx, tag in enumerate(word_pos_tags):\n",
    "    lemmatized_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n",
    "\n",
    "lemmatized_text = \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better than the stemming result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
